\section{Introduction} \seclabel{Intro}

%% what are we trying to say?
% 1. computing semantic diff is important
% 2. computing semantic diff is hard
% 3. existing approaches for computing semantic diff suck
% 4. our approach is great
% 5. technically, we use the following ideas:
%    - correlating program
%    - correlating abstract domains

% TODO:
% - we should have a clear problem definition somewhere
% - what is so special about a patched program?

When applying a patch to a procedure, the programmer has very limited means
for acquiring a description of the change the patch made to the procedure
behavior.

%% problem definition
Given a program $P$ and a patched version of the program $P'$ our goal is to
determine the difference between \semp{P} and \semp{P'}. 

\para{Existing Techniques}
Existing techniques for proving patch equivalence will only supply the
programmer with a binary answer~\cite{GodlinStrichman} as to the
(input-output) equivalence a program but no description of the difference is
supplied. Further work~\cite{KawaguchiLahiriRebelo10} allows refining the
equivalence proof by providing a set of constraints under which equivalence
is desired but requires the programmer to manually deduce these. Other
techniques for describing difference
~\cite{DwyerElbaumPerson08,HawblitzelKawaguchiLahiriRebelo12} which rely on
symbolic execution supply unsound results as they are limited by loops and
essentially cover a subset of program behavior. We present a novel approach
which allows for a sound description of difference for programs with loops.
Our technique employs methods of abstract interpretation for
over-approximating the difference in behaviors, by focusing the abstraction
on the \emph{relationships} between program behaviors i.e. between variables
values (data) and conditionals (path) in the two versions.

In contrast to existing techniques, our approach allows checking for
equivalence in every point of execution, and for every variable, while
previous approaches focus only on input-output equivalence. This enables
detection of key differences that impact the correctness of the patch: if the
changed behavior includes a bug manifested by a local variable (for instance:
array index out of bounds), we will detect and describe it while previous
work only detected it when propagated to the output and equivalence may have
been reported although a bug was introduced. This also provides a challenge
as we need to carefully choose the program locations where we check for
difference otherwise we will spuriously detect difference.

\para{Correlating Program}
Abstracting relationships allows us to maintain focus on difference while
omitting (whenever necessary for scalability) parts of the behavior that does
not entail difference. In order to monitor these relationships we created a
\emph{correlating program} which captures the behavior of both the original
program and its patched version. Instead of designing a correlating semantics
that is capable of co-executing two programs, we chose to automatically
construct the correlating program such that we can benefit from the use of
standard analysis frameworks for analyzing the resulting program. Another
advantage of this new construct, is that you may apply other methods for
equivalence checking directly on it~\cite{EnglerRamos11} as the correlation
allows for a much more fine-grained equivalence checking (between local
variables and not only output).

\para{Correlating Abstractions}
Our abstraction holds data of both sets of variables, joined together and is
initialized to hold equality over all matched variables. This means we can
reflect relationships without necessarily knowing the actual value of a
variables (we can know that $x_old = x_new$ even though actual values are
unknown). We ran out analysis over the correlating program while updated the
domain to reflect program behavior.

To establish equivalence between correlated variables and precisely capture
differences, our domain has to maintain correlating information even when
other information is abstracted away.

Since some updates may result in non-convex information (e.g. taking  a
condition of the form $x \neq 0$ into account), our domain has to represent
non-convex information, at least temporarily. We address this by working with
a powerset domain of a convex representation. To avoid exponential blowup,
our join operator may over-approximate numerical information as long as
equivalence between correlated variables is preserved.

In some cases, it would have been sufficient to use alternative domains that
are capable of representing richer information, such as interval
polyhedra~\cite{CMWC:SAS09}, or other numerical domains that can represent
non-convex information (e.g., \cite{TODO}). The recent donut
domain~\cite{GIBMG:VMCAI12} may be of particular interest for this purpose.
However, the general principle of having to preserve correlating information
even when information about the values is abstracted away, holds in all of
these cases.

%% Talk about the analysis and the correlating domain

%% Talk about why the domain needs to be a power set

%% Talk about the way it was minimized while keeping correlation data

%% Talk about the way it was widened

%% Talk about how the delta is computed

In this paper, we present a technique based on abstract interpretation, that
is able to compute an over-approximation of the difference between numerical
programs or establish their equivalence when no difference exits. The
approach is based on two key ideas: (i)~create a \emph{union program} that
captures the behavior of both the original program and its patched version;
(ii)~analyze the union program with a \emph{correlating domain} that captures
relationships between values of variables in the original program and values
of variables in the patched version.

The idea of a union program is similar to that of
self-composition~\cite{BartheDArgenioRezk04,AikenTerauchi05}, but the way in
which statements in the union program are combined is carefully designed to
keep the steps of the two programs close to each other. Rather than having
the patched program sequentially composed after the original program, our
union program interleaves the two versions. Analysis of the union program can
then recover equivalence between values of correlated variables even when
equivalence is \emph{temporarily} violated by an update in one version, as
the corresponding update in the other version follows shortly thereafter.

\TODO{check whether Aiken paper SAS'05 does some sort of interleaving as well}

\TODO{also need to say that there is the problem of choosing differencing points}

\DONE{there is some term they used in the security literature? self-composition? cite Nauman}

\subsection{Main Contributions}
The main contributions of this paper are as follows:
\begin{itemize}
\item we phrase the problem of semantic differential analysis as an analysis of a union program --- a single program that
    represents an original program and its patched version.
\item we present an approach for analyzing differences over the union program using a correlating abstract domain. Our
    approach is sound --- if there is a difference at a differentiation point, we cannot miss it. However, since we
    over-approximate differences, our approach may report false differences due to approximation.
\item We have implemented our approach in a tool based on the LLVM compiler infrastructure and the APRON numerical abstract
    domain library, and evaluated it using over 50 patches from open-source software including GNU core utilities, Mozilla
    Firefox, and the Linux Kernel. Our evaluation shows that the tool often manages to establish equivalence, reports useful
    approximation of semantic differences when differences exists, and reports only a few false differences.
\end{itemize}


\TODO{mention classical work like \cite{Horwitz:PLDI90,Horwitz:TOPLAS89}} 
