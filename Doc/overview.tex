\section{Overview} \seclabel{Overview}

In this section, we informally describe our approach with a simple example program.

\subsection{Motivating Example}

\figref{MotivatingLoopExample} shows a procedure $P$ and two patched versions of it: $P_1$ - an equivalent refactored version, and $P_2$  which introduces a bug fix (changing program behavior). We aim to find semantic differences between the original $P$ and each of its patched versions.
Ideally, when comparing $P$ and $P_1$ we would be able to establish their equivalence, and when comparing $P$ and $P_2$ (by location) we would be able to observe that:
\begin{itemize}
\item \textbf{(1)}: $s\_len_{P} \geq 0$ but $s\_len_{P2} > 0$.
\item \textbf{(1)}: $i_{P} \geq -1$ but $i_{P2} > -1$.
\item \textbf{(2)}: $i_{P} \neq 0$ but $i_{P2} > -1$.
\item \textbf{(3)}: no difference (except for the difference in \scode{s\_len} already reported in \textbf{(1)}).
\end{itemize}

\begin{figure*}
\centering
\begin{tabular}{ccc}
\begin{lstlisting}
void foo(int arr[],
    unsigned len) {
 int i = len;
 i--;
    (*@\textbf{(1)}@*)
 while (i) {
    (*@\textbf{(2)}@*)
  arr[i] = i;
  i--;
 }
    (*@\textbf{(3)}@*)
}
\end{lstlisting}
&
\begin{lstlisting}
void foo(int arr[],
    unsigned len) {
 int i = len;
 i--;
    (*@\textbf{(1)}@*)
 while (i) {
    (*@\textbf{(2)}@*)
(*@  \textbf{arr[i] = i--;}   @*)
 }
    (*@\textbf{(3)}@*)
}
\end{lstlisting}
&
\begin{lstlisting}
void foo(int arr[],
    unsigned len) {
 int i = len;
(*@ \textbf{if (len == 0) return;} @*)
 i--;
    (*@\textbf{(1)}@*)
 while (i) {
    (*@\textbf{(2)}@*)
  arr[i] = i;
  i--;
 }
    (*@\textbf{(3)}@*)
}
\end{lstlisting}
\\
\small{P} & \small{P1} & \small{P2} \\
\end{tabular}
\caption{Example looping code and patched versions}
\figlabel{MotivatingLoopExample}
\end{figure*}

We hold off addressing the question of how these program locations, which we call \emph{Differencing Points} were selected and matched to produce an optimal result as it is an outcome of our program correlation algorithm later discussed in \secref{CorrelatingProgram}.

\para{Separate Analysis is not Sound}
To achieve this result using abstract interpretation, one might consider performing a separate analysis on each of the procedures and afterwards comparing the results (at the selected program points) to check for difference. However, as we are dealing with over-approximations, it would be impossible to claim equivalence based on two separate abstract states. Say we wish to compare P and P1 at location \textbf{(2)}. Optimally, the separate analysis result at that location will be $\asemp{P} = \asemp{P1} = \{i \neq 0\}$ (we assume the domain may hold non-convex data and elaborate on this issue later on in this section). Though it would be tempting to claim that the programs are equivalent at this point, it would be wrong, as each program may have arrived at the constraints by entirely different means. This can be easily seen by looking at the two loops in \figref{UnequivLoops} where a separate analysis on each will yield the same $\{i \neq 0\}$ result where equivalence clearly does not exist. This key observation, which basically states that \emph{equality under abstraction does not assure concrete equality}, dictates the use of a combined analysis as otherwise we can never hope to establish equivalence.

\begin{figure}
\centering
\begin{tabular}{cc}
\begin{lstlisting}
 while (i) {
  i -= 1;
 }
\end{lstlisting}
&
\begin{lstlisting}
 while (i) {
  i -= 2;
 }
\end{lstlisting}
\end{tabular}
\caption{Example small non-equivalent loops}
\figlabel{UnequivLoops}
\end{figure}

\para{Correlating Program}
One option for performing this combined, correlating analysis is by defining a special correlating semantics which requires performing a dual analysis on both programs, whilst maintaining abstract information regarding the two sets of variables together. This is a viable option, however we chose a different approach where we would simply construct a single correlating program, denoted $P \bowtie P'$ (for the correlation of a program $P$ and it's patched version $P'$), that will hold the variables and statements of the two programs. These will be interleaved in such a way where matching statements (that appear in both versions) will be adjacent, thus allowing the analysis to maintain equivalence. We transform the programs into our own guarded command language beforehand to achieve a better correlation. The variables of the programs are kept separated by tagging all variables from the newer version. A example correlating program for P and P1 from \figref{MotivatingLoopExample} can be seen in \figref{CorrelatedMotivatingLoopExample}. We opted for the correlating program solution since it allows us to employ standard analysis frameworks \cite{CLang} and abstract domains \cite{JeannetMine09}. Another advantage is that the correlating program building process supplies us with a matching of program locations, thus we are able to check for difference at appropriate locations. Lastly, since $P \bowtie P'$ is a syntactically correct program, that contains the semantic of both programs, we are able to use existing techniques of symbolic execution and equivalence checking, as used in previous work \cite{GodlinStrichman09,DwyerElbaumPerson08,EnglerRamos11}, \TODO{to achieve potentially better results}, as we allow for a more fine-grained checking and differencing (as opposed to input-output checking). We will elaborate on these issues, and on the process of building the correlating program in \secref{CorrelatingProgram}.

\begin{figure}
\centering
\lstset{numbers=left}
\begin{lstlisting}
void foo(int arr[], unsigned len) {
    int arr'[] = clone(arr,len);
    int len' = len';
    int i = len;
    int i' = len';
    i--;
    i'--;
    (*@\textbf{(1)}@*)
l:  guard g = (i);
l': guard g' = (i');
    (*@\textbf{(2)}@*)
    if (g) arr[i] = i; (*@ \lnlabel{corloop1} @*)
    if (g') arr'[i'] = i'--; (*@ \lnlabel{corloop2} @*)
    if (g) i--; (*@ \lnlabel{corloop3} @*)
    if (g) goto l;
    if (g') goto l';
    (*@\textbf{(3)}@*)
}
\end{lstlisting}
\caption{$P \bowtie P1$ (from \figref{MotivatingLoopExample})}
\figlabel{CorrelatedMotivatingLoopExamplePP1}
\end{figure}

\para{Correlating Abstract Domain}
Having defined a facility for performing a joint analysis, we need to define our abstraction in such a way where we
are able to maintain equivalence (under abstraction) if such exists, or provide a precise description of the difference while maintaining soundness. Considering a standard relational abstraction, the only case where equivalence in variables is assured, is when both versions of the variable equal the same concrete value. As this is usually not the case (especially for unknown inputs) we explicitly force the abstraction to initially assume equivalence until proven otherwise. For example, when analyzing \figref{CorrelatedMotivatingLoopExamplePP1}, while forcing initial equivalence, we will arrive at location \textbf{(2)} (the start of the loop) knowing that ${i = i'}$. The analysis will then advance over lines \lnref{corloop1} and \lnref{corloop2} and will temporarily lose equivalence but keep the ${i = i' + 1}$ constraint which will be used to restore equivalence as it moves past \lnref{corloop3}. This is a key feature of our analysis where we allow \emph{temporary equivalence divergence} with the ability to later restore it.

\para{Correlating Abstract Domain Must Be Non-Convex}
As stated, explicit equivalence over abstraction is key for proving equivalence. Consequently, we cannot use a standard convex abstract domain \cite{JeannetMine09}, as it will result in lose of equivalence over conditionals. Let us examine the two pieces of equivalent code in \figref{ConditionalExample}. The optimal result here would be that $\asemp{P \bowtie P'} \models \equiv_V$, but the use of a convex domain here can never yield this result. This is due to the fact that when we analyze C1's conditional, and get a state for each path: $\sigma_T = \{x>3,y=4\}$ and $\sigma_F = \{x<=3,y=y'\}$, the following join operation under the convex domain will result in loss of all equivalence information as $\sigma_T \sqcup \sigma_F = \top$ in a convex domain. As a solution, our domain allows delaying the join operation to a later point. This means that our domain is basically a powerset domain, holding a set of convex sub-states, and our join operation is simply set union. Performing the same analysis, now with the new domain, will result in a super-state $\Sigma_1 = [\{x>3,y=4\},\{x<=3,y=y'\}]$ right after $C1 \bowtie C2$'s \lnref{ConditionalExampleLine1}. The analysis will now be able to restore analysis as it will pass \lnref{ConditionalExampleLine2} to update $\Sigma_2 = [\{x>3,y=4,z'=4\},\{x<=3,y=y',z'=4\}]$ and finally after analyzing the second conditional in \lnref{ConditionalExampleLine3}, we will receive $\Sigma_3 = [\{x>3,y=4,z'=4,y'=4\},\{x<=3,y=y',z'=4\}]$ which holds equivalence (this required pruning of unfeasible sub-states, like ones where $x<=3$ and $x>3$ which we perform). \COMMENT{Essentially, the new super-state holds a sub-state (standard convex) for each path in $C1 \bowtie C2$.} An appropriate concern at this point would be exponential explosion due to the potential growth in sub-states at each conditional which will be discuss promptly. An important added benefit of the powerset domain is in precision: since we keep much more refined data regarding variable correlation, we will be able to produce a more precise description of the difference. An indication for this can be drawn from \figref{PrExample}, where using the new domain will give us a refined state which we will later use to produce a precise description of difference, something that we could not achieve with the standard convex domain.

\begin{figure}
\centering
\begin{tabular}{ccc}
\begin{lstlisting}
if (x>3) y = 4;
\end{lstlisting}
&
\begin{lstlisting}
z = 4;
if (x>3) y = z;
\end{lstlisting}
&
\begin{lstlisting}
if (x>3) y = 4;     (*@ \lnlabel{ConditionalExampleLine1} @*)
z' = 4;             (*@ \lnlabel{ConditionalExampleLine2} @*)
if (x'>3) y' = z';  (*@ \lnlabel{ConditionalExampleLine3} @*)
\end{lstlisting}
\\
\small{C1} & \small{C2} & \small{$C1 \bowtie C2$}
\end{tabular}
\caption{Example conditional code and patched version}
\figlabel{ConditionalExample}
\end{figure}

\begin{figure}
\centering
\begin{lstlisting}
  if (width < 0 && input_position == 0)
    {
      chars = 0;
      input_position = 0;
    }
  else if (width < 0 && input_position <= -width)
    input_position = 0;
  else
    input_position += width;
\end{lstlisting}
\caption{Patch taken from corutils 6.11 pr.c}
\figlabel{PrExample}
\end{figure}

\para{Reducing Powerset State - Canonization}
Performing analysis with the powerset domain does not scale as the number of paths in the correlated program may be exponential. We must allow for reduction of super-state $\Sigma = [\sigma_1,...,\sigma_n]$ with acceptable loss of precision. This reduction, or canonization as we call it, can be achieved by joining the sub-states $\sigma_i$ (using the standard precision losing join of the sub-domain) but to perform this we must first answer the following: (i) Which of the sub-states shall be joined together i.e. what is the \emph{Canonization Strategy} and (ii) At which program locations should the canonization occur i.e. what is the \emph{Canonization Point}. A trivial canonization strategy (we name \emph{Join-All}) is simply reverting back to the sub-domain by applying the join on all sub-states which may result in unacceptable precision loss as exemplified in \figref{ConditionalExample}. However, by taking a closer look at the final state of the same example $\Sigma_3 = [\{x>3,y=4,z'=4,y'=4\},\{x<=3,y=y',z'=4\}]$, one may observe that were we to join the two sub-states in $\Sigma_3$ at this point, we would be able to preserve the coveted $y=y'$ constraint (with the acceptable cost of losing the $x$ related constraints). This led us to devise a canonization strategy (we name \emph{Join-Equiv}) that partitions sub-states by the set of variables which they preserve equivalence for. This bounds the super-state size at $2^{|V|}$, where $V$ is the set of correlating variables we wish to track. As mentioned, another key factor in preserving equivalence and maintaining precision is the program location at which the canonization occurs. The first possibility, which is somewhat symmetric to the first canonization strategy, is to canonicalize at every join point i.e. after every branch converges. We name this canonization point selection as \emph{At-Join}. A quick look at \figref{ConditionalExample}'s state after processing \lnref{ConditionalExampleLine1} - $\Sigma_1 = [\{x>3,y=4\},\{x<=3,y=y'\}]$, while applying canonization strategy 1, shows that this may perform badly as we will lose all data regarding $y$. However if we could delay the canonization to a point where the two programs "converge" (at the end), we will get a more precise result which preserves equivalence. We use our \emph{differencing points} as these convergence points and delay applying the canonization strategy until then. We name this canonization point selection as \emph{At-Diff}. Our evaluation includes applying each of our strategies along with each of the canonization points. Intuitively, the results should range from least precise using the <Join-All,At-Join> strategy and point to most precise in the <Join-Equiv,At-Diff> scenario and this is indeed the case as we show in \secref{Evaluation} (not taking into account the no-canonization scenario which is naturally most precise).

\para{Handling Loops - Widening $\nabla^*$}
In order for our analysis to handle loops we require a means for reaching a fixed point. As our analysis advances over a loop and state is transformed, it may keep changing and never converge unless we apply the widening operator to further over-approximate the looping state and arrive at a fixed point. We have the widening operator of our sub-domain at our disposable, but again we are faced with the question of how we apply this operator, i.e. which sub-states $\sigma_i$ from $\Sigma$ should be widened with which $\sigma'_j$ in $\Sigma'$ or which \emph{Widening Strategy} should we apply. A first viable strategy, similar to the first canonization strategy, is to perform an overall join operation on all $\sigma_i$ and $\sigma'_j$ to result is a single convex sub-state and then simply applying the widening to these sub-states using the sub-domain's $\nabla$ operator. If we examine applying this strategy to $P \bowtie P1$ from \figref{CorrelatedMotivatingLoopExamplePP1}, we see that it may successfully arrive at a fixed point that also maintains equivalence. Now let us try applying the strategy to $P \bowtie P2$ as seen in \figref{CorrelatedMotivatingLoopExamplePP2}. First we mention that as P2 introduces a return statement under the $len=0$ condition, the example shows an extra $r$ guard for representing a return (this exists in all GCL programs but we omitted it so far for brevity). While analyzing, once we pass \lnref{CorrelatedMotivatingLoopExamplePP2Line1}, our state is split to reflect the return effect $\Sigma = [\sigma_1 = \{i' = len' = 0, r = 1, r' = 0, \equiv_{i,len}\},\{i' = len', i' \neq 0, \sigma_2 = \equiv_{r,i,len}\}$. As we further advance into the loop, $\sigma_2$ will maintain equivalence (and in fact will not require widening?) but $\sigma_1$ will continue to update the part of the state regarding un-tagged variables  (since $r'=1$ in $\sigma_1$ and it will not consider any of the commands guarded by $r'$), specifically it will decrement $i$ continuously, preventing the analysis from reaching fixed point. We would require widening here but using the naive strategy of a complete join will result in aggressive loss of precision, specifically losing all information regarding $i$. The problem originates from the fact that prior to widening, we joined sub-states which adhere to two different loop behaviors: one where both P and P2 loop together (that originated form $len \neq 0$) and the other where P2 has existed but P continues to loop ($len = 0$). Ideally, we would like to match these two behaviors and widen them accordingly. We devised a widening strategy that allows to do this as it basically matches sub-states that adhere to the same behavior, or loop-paths. We do this by using guard for the matching. If two sub-states agree on their set of guards, it means they represent the same loop path and can be widened as the latter originated from the former (widening operates on subsequent iterations). In our example, using this strategy will allow for a precise description of the difference conserved in the state $\sigma_1 = {len = len' =0, -\inf < i < 0, i' = 0, r = 1, r' = 0}$ showing that the original program continued to access the array at illegal $-\inf < i < 0$ indexes, where P2 stopped and exited. We also define the notion of \emph{Widening Point} meaning at what point in the loop we widen and experiment with applying at back-edges and differencing points. \TODO{maybe add a nice picture of behaviors in P and P2 and how they are widened}.

\begin{figure}
\centering
\lstset{numbers=left}
\begin{lstlisting}
void foo(int arr[], unsigned len) {
    guard r = 1;
    guard r' = 1;    
    int arr'[] = clone(arr,len);
    int len' = len';
    int i = len;
    int i' = len';
    if (r') if (len' == 0) r' = 0; (*@ \lnlabel{CorrelatedMotivatingLoopExamplePP2Line1} @*)
    (*@\textbf{(1)}@*)    
    if (r) i--;
    if (r') i'--;
l:  guard g = 0;
l': guard g' = 0;
    if (r) g = (i);
    if (r') g' = (i');
    if (r) if (g) arr[i] = i;
    if (r') if (g') arr'[i'] = i';
    if (r) if (g) i--;
    if (r') if (g') i'--;
    if (r) if (g) goto l;
    if (r') if (g') goto l';
}
\end{lstlisting}
\caption{$P \bowtie P2$ (from \figref{MotivatingLoopExample})}
\figlabel{CorrelatedMotivatingLoopExamplePP2}
\end{figure}

\para{Producing Diff}
Finally, we need to give meaning to state difference and find an abstraction for precisely capturing the difference. \TODO{add pretty diff calculation picture from poster}

\TODO{knowing equivalence is important, but knowing the actual difference (abstract characterization of it) is useful} 