\section{Related Work}\label{Sec:Related}

\paragraph{Bounded symbolic execution in CLang}
As prior work we used the CLang infrastructure \cite{CLang} static analysis graph reachability engine in order to perform a simple and bounded state differentiation exploration. We used the existing infrastructure and it's abstract representation facilities to simply record every location where the 2 versions of the variables differ. This of course was not sufficient since it only presents a bounded solution and we will show the limitations of this method by example.

\paragraph{Existing work on patch-based exploit generation}
Brumley, Poosankam, Song and Zheng \cite{AutoPatch} is the prominent work addressing patch-based analysis. We differ from this work in the following aspects:
\begin{enumerate}
\item First, the problem definition in said work is different from our own. They aim to find an $exploit$ for vulnerabilities fixed by a certain patch. Furthermore, this exploit is defined in relevance to a $security policy$ which can differ. While our goals are similar to those of \cite{AutoPatch}, we achieve them by solving 2 extended problems of a) recording the delta between new variable values and old ones and b) producing input from said values. These problems are a superset of the problem described in \cite{AutoPatch} and solving them has the potential for a much more complete and sound result.
\item We aim to find differentiation between every variable changed by the patch and analyze that differentiation while they concentrate on input sanitation alone. Thus if in the patched program some variable has changed in a way that does not involve input validation, it will be disregarded: for instance if an array index variable $i$ to a buffer $B$ is patched by adding an assignment $i = sizeof(B) - 1$, it will be ignored in the previous work while we will record that the old version of $i$ can no longer have values greater than $sizeof(B) - 1$ and may use it for exploit generation.
\item We perform our analysis on the source code of the program and patch instead of the binary. Working on a higher level gives us much more data thus potentially allowing for more results.
\end{enumerate}

Kroening and Heelan \cite{AutoExploit} main focus focus was producing an exploit from given input that is known to trigger a bug. No patch is involved in the process. Our
goal is to produce said input from the corrected software thus \cite{AutoExploit} can be used to create an exploit from our results.

Song, Zhang and Sun \cite{AutoBinary} also relate to the patch-based exploit generation problem but their main focus is on finding similarities between versions of the binary to better couple functions from the original program with their patched counter-part, a problem that was not addressed in \cite{AutoPatch}. Also their method of recognizing possible exploits is degenerate and relies on identifying known input validation functions that were added to a certain path - a method that could be easily overcome.

Oh \cite{DarunGrim3} presented a new version for the DarunGrim binary diffing tool aimed at better reviewing patched programs and specifically finding patches with security implications. The goal of the tool is to help researches who manually scan patches for the purpose of producing intrusion prevention system signatures. The tool relies mainly on syntactic analysis of patterns to produce a security implication score for procedures patches making them a candidate for manual inspection. \cite{AutoPatch} used the DarunGrim binary diffing tool EBDS for their experiment.

Person, Dwyer, Elbaum and Pasareanu~\cite{DEP:FSE08} introduced an extension and
application of symbolic execution techniques that computes a
precise behavioral characterization of a program change called
differential symbolic execution. As we also implemented bounded
symbolic execution as our preliminary work we will discuss this
method in comparison to our own.

Godlin and Strichman~\cite{GS09} developed a
method for proving the equivalence of similar C programs under
certain restrictions based on and existing functional verification
tool. This was a basis for future work regarding equivalence and
we intend to base our work upon these advances.

Kawaguchi, Lahiri and Rebelo~\cite{LHKR:CAV12} defined the concept of
\textit{conditional equivalence} meaning under which conditions
(inputs) are 2 different versions of a program equivalent (i.e.
produce the same output). Their goal is to keep software changes
from breaking procedure contracts and changing module behavior too
drastically and they achieve this by computing the conditions
under which the behavior is preserved. This work indirectly
addresses our problem and we believe we can leverage their
techniques for producing the inputs that break the equivalence
while focusing on bug triggering rather than contract breaking.


\TODO{ \cite{GS:DAC09} }


\paragraph{Determining corresponding components}

As suggested in \cite{Horwitz:PLDI90}, one possibility is to rely on the editing sequence that creates the new version from the original one. Another option is using various syntactic differencing algorithms as a base for computing correspondence tags.

\TODO{their idea for computing correspondence, is to minimize the ``size of change''. They have two different notions of size of change.}

\cite{ARRSY:CAV07} introduced a correlating heap semantics for verifying linearizability of concurrent programs. In their work, a correlating heap semantics is used to establish correspondence between a concurrent program and a sequential version of the program at specific linearization points.  